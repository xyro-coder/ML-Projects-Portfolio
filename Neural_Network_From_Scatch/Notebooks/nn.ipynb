{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import standard libraries\n",
    "import torch\n",
    "import sklearn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.datasets\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "encoder = OneHotEncoder()\n",
    "def load_data():\n",
    "    mnist = sklearn.datasets.fetch_openml('mnist_784')\n",
    "    X, y = np.array(mnist.data), np.array(mnist.target).reshape(-1, 1)\n",
    "    return X, encoder.fit_transform(y).toarray()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784,)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network:\n",
    "    def __init__(self, sizes, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        np.random.seed(4)\n",
    "        # The list ``sizes`` contains the number of neurons in the respective layers of the network.\n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        # This create an array of random integers of shape (y, 1), y being the number of neuron in a layer\n",
    "        # We can also see another observation which is for every layer we get of length y we get a vector\n",
    "        # Each entry in this vector of the bias of one neuron\n",
    "        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
    "        self.weights = [np.random.randn(y, x) for x, y in zip(sizes[:-1], sizes[1:])]\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        x = np.clip(x, -500, 500)\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def Forward_propogation(self):\n",
    "        self.a = self.X.T\n",
    "        self.previous_activation = []\n",
    "        \n",
    "        for layer in range(0, len(self.sizes) - 1):\n",
    "            z = np.dot(self.weights[layer], self.a) + self.biases[layer]\n",
    "            if layer == len(self.sizes) - 2:\n",
    "                z_max = np.max(z, axis=0, keepdims=True)\n",
    "                self.a = np.exp(z - z_max) / np.sum(np.exp(z - z_max), axis=0)\n",
    "            else:\n",
    "                self.a = self.sigmoid(z)\n",
    "            self.previous_activation.append(self.a)\n",
    "        return self.a\n",
    "    def Cost(self):\n",
    "        # we will use categorical cross entorpy \n",
    "        y = self.y\n",
    "        epsilon = 10 ** -15\n",
    "        y_pred = self.a.T\n",
    "        y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "        return  -1 * np.sum((y * np.log(y_pred))) / len(y)\n",
    "    def backwards(self):\n",
    "        delta = (self.a - self.y.T)\n",
    "        weights = []\n",
    "        biases = []\n",
    "        for layer in range(len(self.weights) - 1, -1, -1):\n",
    "            if layer > 0:\n",
    "                a_prev = self.previous_activation[layer - 1]  \n",
    "            if layer == 0:\n",
    "                a_prev = self.X.T  \n",
    "            grad_weights = np.zeros(self.weights[layer].shape)\n",
    "            grad_bias = np.zeros(self.biases[layer].shape)\n",
    "            grad_weights = np.dot(delta, a_prev.T) \n",
    "            grad_bias = np.sum(delta, axis=1, keepdims=True)\n",
    "            if layer > 0:\n",
    "                delta = np.dot(self.weights[layer].T, delta) * (a_prev * (1 - a_prev))\n",
    "            weights.append(grad_weights)\n",
    "            biases.append(grad_bias)\n",
    "        self.grad_weights = weights[::-1]\n",
    "        self.grad_biases = biases[::-1]\n",
    "        return self.grad_weights, self.grad_biases\n",
    "\n",
    "    def optimizer(self, learning_rate=0.01 / 3, training_epochs=200, beta1=0.99, beta2= 0.999):\n",
    "        t = 1\n",
    "        epsilon = 1e-6\n",
    "        weights_moments = [np.zeros(weight.shape) for weight in self.weights]\n",
    "        biases_moments = [np.zeros(weight.shape) for weight in self.biases]\n",
    "        weights_velocities = [np.zeros(weight.shape) for weight in self.weights]      \n",
    "        biases_velocities = [np.zeros(weight.shape) for weight in self.biases]\n",
    "        for _ in range(training_epochs):\n",
    "            self.Forward_propogation()\n",
    "            self.backwards()\n",
    "            for layer in range(len(self.weights)):\n",
    "                # weights_velocities[layer] = weights_velocities[layer] + (self.grad_weights[layer] * self.grad_weights[layer])\n",
    "                # biases_velocities[layer] = biases_velocities[layer]+ (self.grad_biases[layer] * self.grad_biases[layer])\n",
    "                weights_moments[layer] = beta1 * weights_moments[layer] + (1 - beta1) * self.grad_weights[layer]\n",
    "                biases_moments[layer] = beta1 * biases_moments[layer] + (1 - beta1) * self.grad_biases[layer]\n",
    "                weights_velocities[layer] = beta2 * weights_velocities[layer] + (1 - beta2) * (self.grad_weights[layer] * self.grad_weights[layer]) \n",
    "                biases_velocities[layer] = beta2 * biases_velocities[layer]+ (1 - beta2) * (self.grad_biases[layer] * self.grad_biases[layer])\n",
    "                weights_moments_prime = weights_moments[layer] / (1 - beta1 ** t)\n",
    "                biases_moments_prime = biases_moments[layer] / (1 - beta1 ** t)\n",
    "                weights_velocities_prime = weights_velocities[layer] / (1 - beta2 ** t)\n",
    "                biases_velocities_prime = biases_velocities[layer] / (1 - beta2 ** t)\n",
    "                self.weights[layer] -= learning_rate * weights_moments_prime / np.sqrt(weights_velocities_prime+ epsilon)\n",
    "                # self.weights[layer] -= learning_rate * self.grad_weights[layer]\n",
    "                self.biases[layer] -= learning_rate * biases_moments_prime  / np.sqrt(biases_velocities_prime  + epsilon)\n",
    "            t += 1\n",
    "            print(f\"Epoch {_}/{training_epochs} | Iteration {_} | Cost: {self.Cost():.6f}\")\n",
    "\n",
    "        #     grad_norms = [np.linalg.norm(gw) for gw in self.grad_weights]\n",
    "        #     exploading_threshold = 1.0\n",
    "        #     vanishing_threshold = 1e-6\n",
    "        #     if all(norm < vanishing_threshold for norm in grad_norms):\n",
    "        #         learning_rate *=5 \n",
    "        #     if all(norm > exploading_threshold for norm in grad_norms):\n",
    "        #         learning_rate *= 0.5\n",
    "        # if abs(min(grad_norms) - max(grad_norms)) < 1e-8:  # Floating-point tolerant comparison\n",
    "        #     print(f\"CRITICAL: All gradients have identical magnitude ({grad_norms[0]:.6e})\")\n",
    "        #     # Emergency measures\n",
    "        #     learning_rate *= 10  # Drastic LR increase\n",
    "        #     for gw in self.grad_weights:  # Add noise to break symmetry\n",
    "        #         gw += np.random.normal(0, 1e-6, gw.shape)\n",
    "                \n",
    "        \n",
    "    def train(self):\n",
    "        self.Forward_propogation()\n",
    "        print(\"Starting training...\")\n",
    "        print(f\"Initial Cost: {self.Cost():.6f}\")  \n",
    "        # Apply gradient descent with the specified parameters\n",
    "        self.optimizer()\n",
    "        \n",
    "        print(f\"Final Cost: {self.Cost():.6f}\")\n",
    "        print(\"Training complete!\")\n",
    "    def accuracy(self):\n",
    "        preds = np.argmax(self.a.T, axis=1)\n",
    "        targets = np.argmax(self.y, axis=1)\n",
    "        return np.mean(preds == targets)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X / 255.0  # Scale pixel values to 0-1 range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = Network([784, 256, 128, 10], X, y)  # More efficient gradient flow\n",
    "#nn = Network([784, 128, 10], X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Initial Cost: 7.749283\n",
      "Epoch 0/200 | Iteration 0 | Cost: 7.749283\n",
      "Epoch 1/200 | Iteration 1 | Cost: 6.248049\n",
      "Epoch 2/200 | Iteration 2 | Cost: 5.343812\n",
      "Epoch 3/200 | Iteration 3 | Cost: 4.782090\n",
      "Epoch 4/200 | Iteration 4 | Cost: 4.394985\n",
      "Epoch 5/200 | Iteration 5 | Cost: 4.097846\n",
      "Epoch 6/200 | Iteration 6 | Cost: 3.846191\n",
      "Epoch 7/200 | Iteration 7 | Cost: 3.618595\n",
      "Epoch 8/200 | Iteration 8 | Cost: 3.404706\n",
      "Epoch 9/200 | Iteration 9 | Cost: 3.197660\n",
      "Epoch 10/200 | Iteration 10 | Cost: 2.992148\n",
      "Epoch 11/200 | Iteration 11 | Cost: 2.785703\n",
      "Epoch 12/200 | Iteration 12 | Cost: 2.580287\n",
      "Epoch 13/200 | Iteration 13 | Cost: 2.381518\n",
      "Epoch 14/200 | Iteration 14 | Cost: 2.196442\n",
      "Epoch 15/200 | Iteration 15 | Cost: 2.031337\n",
      "Epoch 16/200 | Iteration 16 | Cost: 1.890116\n",
      "Epoch 17/200 | Iteration 17 | Cost: 1.773552\n",
      "Epoch 18/200 | Iteration 18 | Cost: 1.679662\n",
      "Epoch 19/200 | Iteration 19 | Cost: 1.604822\n",
      "Epoch 20/200 | Iteration 20 | Cost: 1.544847\n",
      "Epoch 21/200 | Iteration 21 | Cost: 1.495636\n",
      "Epoch 22/200 | Iteration 22 | Cost: 1.453448\n",
      "Epoch 23/200 | Iteration 23 | Cost: 1.415234\n",
      "Epoch 24/200 | Iteration 24 | Cost: 1.378950\n",
      "Epoch 25/200 | Iteration 25 | Cost: 1.343626\n",
      "Epoch 26/200 | Iteration 26 | Cost: 1.309148\n",
      "Epoch 27/200 | Iteration 27 | Cost: 1.275906\n",
      "Epoch 28/200 | Iteration 28 | Cost: 1.244402\n",
      "Epoch 29/200 | Iteration 29 | Cost: 1.214920\n",
      "Epoch 30/200 | Iteration 30 | Cost: 1.187387\n",
      "Epoch 31/200 | Iteration 31 | Cost: 1.161382\n",
      "Epoch 32/200 | Iteration 32 | Cost: 1.136257\n",
      "Epoch 33/200 | Iteration 33 | Cost: 1.111331\n",
      "Epoch 34/200 | Iteration 34 | Cost: 1.086061\n",
      "Epoch 35/200 | Iteration 35 | Cost: 1.060162\n",
      "Epoch 36/200 | Iteration 36 | Cost: 1.033636\n",
      "Epoch 37/200 | Iteration 37 | Cost: 1.006736\n",
      "Epoch 38/200 | Iteration 38 | Cost: 0.979890\n",
      "Epoch 39/200 | Iteration 39 | Cost: 0.953607\n",
      "Epoch 40/200 | Iteration 40 | Cost: 0.928401\n",
      "Epoch 41/200 | Iteration 41 | Cost: 0.904726\n",
      "Epoch 42/200 | Iteration 42 | Cost: 0.882927\n",
      "Epoch 43/200 | Iteration 43 | Cost: 0.863220\n",
      "Epoch 44/200 | Iteration 44 | Cost: 0.845682\n",
      "Epoch 45/200 | Iteration 45 | Cost: 0.830274\n",
      "Epoch 46/200 | Iteration 46 | Cost: 0.816867\n",
      "Epoch 47/200 | Iteration 47 | Cost: 0.805270\n",
      "Epoch 48/200 | Iteration 48 | Cost: 0.795252\n",
      "Epoch 49/200 | Iteration 49 | Cost: 0.786564\n",
      "Epoch 50/200 | Iteration 50 | Cost: 0.778959\n",
      "Epoch 51/200 | Iteration 51 | Cost: 0.772208\n",
      "Epoch 52/200 | Iteration 52 | Cost: 0.766105\n",
      "Epoch 53/200 | Iteration 53 | Cost: 0.760470\n",
      "Epoch 54/200 | Iteration 54 | Cost: 0.755143\n",
      "Epoch 55/200 | Iteration 55 | Cost: 0.749992\n",
      "Epoch 56/200 | Iteration 56 | Cost: 0.744904\n",
      "Epoch 57/200 | Iteration 57 | Cost: 0.739791\n",
      "Epoch 58/200 | Iteration 58 | Cost: 0.734579\n",
      "Epoch 59/200 | Iteration 59 | Cost: 0.729208\n",
      "Epoch 60/200 | Iteration 60 | Cost: 0.723629\n",
      "Epoch 61/200 | Iteration 61 | Cost: 0.717796\n",
      "Epoch 62/200 | Iteration 62 | Cost: 0.711672\n",
      "Epoch 63/200 | Iteration 63 | Cost: 0.705224\n",
      "Epoch 64/200 | Iteration 64 | Cost: 0.698433\n",
      "Epoch 65/200 | Iteration 65 | Cost: 0.691289\n",
      "Epoch 66/200 | Iteration 66 | Cost: 0.683793\n",
      "Epoch 67/200 | Iteration 67 | Cost: 0.675960\n",
      "Epoch 68/200 | Iteration 68 | Cost: 0.667811\n",
      "Epoch 69/200 | Iteration 69 | Cost: 0.659380\n",
      "Epoch 70/200 | Iteration 70 | Cost: 0.650711\n",
      "Epoch 71/200 | Iteration 71 | Cost: 0.641854\n",
      "Epoch 72/200 | Iteration 72 | Cost: 0.632872\n",
      "Epoch 73/200 | Iteration 73 | Cost: 0.623830\n",
      "Epoch 74/200 | Iteration 74 | Cost: 0.614801\n",
      "Epoch 75/200 | Iteration 75 | Cost: 0.605857\n",
      "Epoch 76/200 | Iteration 76 | Cost: 0.597076\n",
      "Epoch 77/200 | Iteration 77 | Cost: 0.588532\n",
      "Epoch 78/200 | Iteration 78 | Cost: 0.580298\n",
      "Epoch 79/200 | Iteration 79 | Cost: 0.572441\n",
      "Epoch 80/200 | Iteration 80 | Cost: 0.565021\n",
      "Epoch 81/200 | Iteration 81 | Cost: 0.558086\n",
      "Epoch 82/200 | Iteration 82 | Cost: 0.551671\n",
      "Epoch 83/200 | Iteration 83 | Cost: 0.545800\n",
      "Epoch 84/200 | Iteration 84 | Cost: 0.540481\n",
      "Epoch 85/200 | Iteration 85 | Cost: 0.535709\n",
      "Epoch 86/200 | Iteration 86 | Cost: 0.531471\n",
      "Epoch 87/200 | Iteration 87 | Cost: 0.527742\n",
      "Epoch 88/200 | Iteration 88 | Cost: 0.524492\n",
      "Epoch 89/200 | Iteration 89 | Cost: 0.521681\n",
      "Epoch 90/200 | Iteration 90 | Cost: 0.519266\n",
      "Epoch 91/200 | Iteration 91 | Cost: 0.517198\n",
      "Epoch 92/200 | Iteration 92 | Cost: 0.515424\n",
      "Epoch 93/200 | Iteration 93 | Cost: 0.513888\n",
      "Epoch 94/200 | Iteration 94 | Cost: 0.512536\n",
      "Epoch 95/200 | Iteration 95 | Cost: 0.511310\n",
      "Epoch 96/200 | Iteration 96 | Cost: 0.510155\n",
      "Epoch 97/200 | Iteration 97 | Cost: 0.509019\n",
      "Epoch 98/200 | Iteration 98 | Cost: 0.507854\n",
      "Epoch 99/200 | Iteration 99 | Cost: 0.506615\n",
      "Epoch 100/200 | Iteration 100 | Cost: 0.505263\n",
      "Epoch 101/200 | Iteration 101 | Cost: 0.503767\n",
      "Epoch 102/200 | Iteration 102 | Cost: 0.502101\n",
      "Epoch 103/200 | Iteration 103 | Cost: 0.500245\n",
      "Epoch 104/200 | Iteration 104 | Cost: 0.498189\n",
      "Epoch 105/200 | Iteration 105 | Cost: 0.495927\n",
      "Epoch 106/200 | Iteration 106 | Cost: 0.493460\n",
      "Epoch 107/200 | Iteration 107 | Cost: 0.490794\n",
      "Epoch 108/200 | Iteration 108 | Cost: 0.487940\n",
      "Epoch 109/200 | Iteration 109 | Cost: 0.484911\n",
      "Epoch 110/200 | Iteration 110 | Cost: 0.481727\n",
      "Epoch 111/200 | Iteration 111 | Cost: 0.478407\n",
      "Epoch 112/200 | Iteration 112 | Cost: 0.474971\n",
      "Epoch 113/200 | Iteration 113 | Cost: 0.471442\n",
      "Epoch 114/200 | Iteration 114 | Cost: 0.467842\n",
      "Epoch 115/200 | Iteration 115 | Cost: 0.464192\n",
      "Epoch 116/200 | Iteration 116 | Cost: 0.460513\n",
      "Epoch 117/200 | Iteration 117 | Cost: 0.456824\n",
      "Epoch 118/200 | Iteration 118 | Cost: 0.453144\n",
      "Epoch 119/200 | Iteration 119 | Cost: 0.449489\n",
      "Epoch 120/200 | Iteration 120 | Cost: 0.445872\n",
      "Epoch 121/200 | Iteration 121 | Cost: 0.442307\n",
      "Epoch 122/200 | Iteration 122 | Cost: 0.438804\n",
      "Epoch 123/200 | Iteration 123 | Cost: 0.435370\n",
      "Epoch 124/200 | Iteration 124 | Cost: 0.432012\n",
      "Epoch 125/200 | Iteration 125 | Cost: 0.428734\n",
      "Epoch 126/200 | Iteration 126 | Cost: 0.425539\n",
      "Epoch 127/200 | Iteration 127 | Cost: 0.422428\n",
      "Epoch 128/200 | Iteration 128 | Cost: 0.419401\n",
      "Epoch 129/200 | Iteration 129 | Cost: 0.416459\n",
      "Epoch 130/200 | Iteration 130 | Cost: 0.413601\n",
      "Epoch 131/200 | Iteration 131 | Cost: 0.410827\n",
      "Epoch 132/200 | Iteration 132 | Cost: 0.408136\n",
      "Epoch 133/200 | Iteration 133 | Cost: 0.405530\n",
      "Epoch 134/200 | Iteration 134 | Cost: 0.403008\n",
      "Epoch 135/200 | Iteration 135 | Cost: 0.400570\n",
      "Epoch 136/200 | Iteration 136 | Cost: 0.398217\n",
      "Epoch 137/200 | Iteration 137 | Cost: 0.395950\n",
      "Epoch 138/200 | Iteration 138 | Cost: 0.393767\n",
      "Epoch 139/200 | Iteration 139 | Cost: 0.391670\n",
      "Epoch 140/200 | Iteration 140 | Cost: 0.389657\n",
      "Epoch 141/200 | Iteration 141 | Cost: 0.387729\n",
      "Epoch 142/200 | Iteration 142 | Cost: 0.385882\n",
      "Epoch 143/200 | Iteration 143 | Cost: 0.384115\n",
      "Epoch 144/200 | Iteration 144 | Cost: 0.382426\n",
      "Epoch 145/200 | Iteration 145 | Cost: 0.380810\n",
      "Epoch 146/200 | Iteration 146 | Cost: 0.379263\n",
      "Epoch 147/200 | Iteration 147 | Cost: 0.377781\n",
      "Epoch 148/200 | Iteration 148 | Cost: 0.376356\n",
      "Epoch 149/200 | Iteration 149 | Cost: 0.374984\n",
      "Epoch 150/200 | Iteration 150 | Cost: 0.373656\n",
      "Epoch 151/200 | Iteration 151 | Cost: 0.372366\n",
      "Epoch 152/200 | Iteration 152 | Cost: 0.371105\n",
      "Epoch 153/200 | Iteration 153 | Cost: 0.369865\n",
      "Epoch 154/200 | Iteration 154 | Cost: 0.368639\n",
      "Epoch 155/200 | Iteration 155 | Cost: 0.367417\n",
      "Epoch 156/200 | Iteration 156 | Cost: 0.366193\n",
      "Epoch 157/200 | Iteration 157 | Cost: 0.364957\n",
      "Epoch 158/200 | Iteration 158 | Cost: 0.363703\n",
      "Epoch 159/200 | Iteration 159 | Cost: 0.362423\n",
      "Epoch 160/200 | Iteration 160 | Cost: 0.361113\n",
      "Epoch 161/200 | Iteration 161 | Cost: 0.359767\n",
      "Epoch 162/200 | Iteration 162 | Cost: 0.358380\n",
      "Epoch 163/200 | Iteration 163 | Cost: 0.356950\n",
      "Epoch 164/200 | Iteration 164 | Cost: 0.355475\n",
      "Epoch 165/200 | Iteration 165 | Cost: 0.353954\n",
      "Epoch 166/200 | Iteration 166 | Cost: 0.352387\n",
      "Epoch 167/200 | Iteration 167 | Cost: 0.350776\n",
      "Epoch 168/200 | Iteration 168 | Cost: 0.349122\n",
      "Epoch 169/200 | Iteration 169 | Cost: 0.347430\n",
      "Epoch 170/200 | Iteration 170 | Cost: 0.345703\n",
      "Epoch 171/200 | Iteration 171 | Cost: 0.343948\n",
      "Epoch 172/200 | Iteration 172 | Cost: 0.342169\n",
      "Epoch 173/200 | Iteration 173 | Cost: 0.340372\n",
      "Epoch 174/200 | Iteration 174 | Cost: 0.338566\n",
      "Epoch 175/200 | Iteration 175 | Cost: 0.336756\n",
      "Epoch 176/200 | Iteration 176 | Cost: 0.334950\n",
      "Epoch 177/200 | Iteration 177 | Cost: 0.333154\n",
      "Epoch 178/200 | Iteration 178 | Cost: 0.331375\n",
      "Epoch 179/200 | Iteration 179 | Cost: 0.329620\n",
      "Epoch 180/200 | Iteration 180 | Cost: 0.327895\n",
      "Epoch 181/200 | Iteration 181 | Cost: 0.326205\n",
      "Epoch 182/200 | Iteration 182 | Cost: 0.324554\n",
      "Epoch 183/200 | Iteration 183 | Cost: 0.322946\n",
      "Epoch 184/200 | Iteration 184 | Cost: 0.321385\n",
      "Epoch 185/200 | Iteration 185 | Cost: 0.319872\n",
      "Epoch 186/200 | Iteration 186 | Cost: 0.318410\n",
      "Epoch 187/200 | Iteration 187 | Cost: 0.316997\n",
      "Epoch 188/200 | Iteration 188 | Cost: 0.315635\n",
      "Epoch 189/200 | Iteration 189 | Cost: 0.314322\n",
      "Epoch 190/200 | Iteration 190 | Cost: 0.313055\n",
      "Epoch 191/200 | Iteration 191 | Cost: 0.311833\n",
      "Epoch 192/200 | Iteration 192 | Cost: 0.310652\n",
      "Epoch 193/200 | Iteration 193 | Cost: 0.309508\n",
      "Epoch 194/200 | Iteration 194 | Cost: 0.308398\n",
      "Epoch 195/200 | Iteration 195 | Cost: 0.307316\n",
      "Epoch 196/200 | Iteration 196 | Cost: 0.306259\n",
      "Epoch 197/200 | Iteration 197 | Cost: 0.305221\n",
      "Epoch 198/200 | Iteration 198 | Cost: 0.304198\n",
      "Epoch 199/200 | Iteration 199 | Cost: 0.303186\n",
      "Final Cost: 0.303186\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "nn.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9118714285714286"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
